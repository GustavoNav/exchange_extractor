# üìä **Extract-Exchange**

## üõ†Ô∏è Sobre

O **Extract-Exchange** √© um projeto dedicado ao estudo de Engenharia de Dados. Ele fornece um ambiente robusto para coleta de dados por meio de APIs e Web Scraping, armazenamento dessas informa√ß√µes em um banco de dados PostgreSQL e exibi√ß√£o dos dados atrav√©s de visualiza√ß√µes interativas. Utilizamos **Streamlit** para a demonstra√ß√£o das visualiza√ß√µes e **Airflow** para gerenciar e executar as pipelines.

O projeto foi criado seguindo boas praticas de programa√ß√£o, sendo completamente modularizado, cada recurso pode funcionar independete do outro. 

Para testar o resultado no streamlit sem precisar instalar o reposit√≥rio, acesse: [https://view-exchange-tnthxryusub5cwfncvyhfn.streamlit.app/](https://view-exchange-tnthxryusub5cwfncvyhfn.streamlit.app/)

Siga as instru√ß√µes para fazer o ambiente funcionar e se comunicar por completo.

## üèóÔ∏è Arquitetura

![Imagem da Arquitetura](image.png)

### data_extractor:
Aplica√ß√£o respons√°vel por fazer a coleta de dados. O requeriments.txt possui apenas as configura√ß√µes necess√°ria para essa parte funcionar individualmente, voc√™ pode executar de forma independente utilizando as fun√ß√µes dispon√≠veis no run.py de cada um dos extractors:

- general_information_extractor (Por Web Scraping)
- historic_extractor (Utiliza a API)
- real_time_extractor (Utiliza a API)

O Dockerfile √© para colocar a aplica√ß√£o em um container (√© essencial para funcionar com o docker-compose, caso tente executar separadamente, √© preciso configurar um banco de dados, a conex√£o √© feita utilizando sqlalchemy e pode ser ajustada em data_extractor/db , a inje√ß√£o de dados √© individual para cada extractor, e deve ser configurada no repository dispon√≠vel em data_extractor/nome_do_extractor/db )

#### Como os Dados S√£o Coletados?

Os dados foram coletados de 2 formas:
- Por meio da API [yfinance](https://github.com/ranaroussi/yfinance) para coletar dados financeiros de empresas.

- Por meio de web scraping, que √© realizado com `requests` para obter o HTML de [InfoMoney](https://www.infomoney.com.br/) e, em seguida, s√£o realizadas opera√ß√µes com Regex para extrair os dados das empresas.

### data_visualize
O data_visualize permite ter uma visualiza√ß√£o do dados, utilizando streamlit. Os dados foram salvos no formato parquet, o repository permite coletar os dados atuais do banco de dados para carregar. O requeriments.txt possui as configura√ß√µes para que esse m√≥dulo possa funcionar individualmente. 

J√° tem 3 arquivos parquet salvos no reposit√≥rio para visualiza√ß√£o caso n√£o queira puxar do banco de dados.

Para executar o streamlit utilize:

```
streamlit run ./data_visualize/main.py
```

Por fim acesse a URL disponibilizada no terminal.


## üîß Configura√ß√£o

### 0. Banco de Dados

O banco de dados √© criado automaticamente ao executar o Docker Compose e √© dividido em 3 tabelas:

- **tb_historic**: Armazena dados financeiros da empresa ao longo do tempo. 
- **tb_general_financials**: Armazena dados gerais da empresa ao longo do tempo. 
- **tb_real_time**: Armazena dados financeiros da empresa ao longo do dia, com intervalo de 1 minuto.  

```sql
CREATE TABLE IF NOT EXISTS tb_historic (
    id SERIAL PRIMARY KEY,
    date_information TIMESTAMP WITH TIME ZONE,
    open NUMERIC(14, 6),
    high NUMERIC(14, 6),
    low NUMERIC(14, 6),
    close NUMERIC(14, 6),
    volume BIGINT,
    dividends NUMERIC(14, 6),
    stock_splits NUMERIC(14, 6),
    company_code VARCHAR(10),
    CONSTRAINT unique_historic UNIQUE (date_information, open, high, low, close, volume, dividends, stock_splits, company_code)
);

CREATE TABLE IF NOT EXISTS tb_general_financials (
    id SERIAL PRIMARY KEY,
    company_code VARCHAR(10) NOT NULL,
    sector VARCHAR(50),
    net_sales NUMERIC(15, 2),
    net_income NUMERIC(15, 2),
    net_margin NUMERIC(5, 2),
    ebitda NUMERIC(15, 2),
    ebitda_margin NUMERIC(5, 2),
    total_assets NUMERIC(15, 2),
    gross_debt NUMERIC(15, 2),
    equity NUMERIC(15, 2),
    pe_ratio NUMERIC(5, 2),
    CONSTRAINT unique_company_code UNIQUE (company_code)
);

CREATE TABLE IF NOT EXISTS tb_real_time(
    id SERIAL PRIMARY KEY,
    company_code VARCHAR(10) NOT NULL,
    date_information TIMESTAMP,
    open NUMERIC(14, 6),
    high NUMERIC(14, 6),
    low NUMERIC(14, 6),
    close NUMERIC(14, 6),
    CONSTRAINT unique_real_time UNIQUE (company_code, date_information, open, high, low, close)
);
```
### 1. Configura√ß√£o do Ambiente
Na raiz do projeto, execute o seguinte comando para criar e iniciar as imagens e containers Docker:

```
docker-compose up --build
```

Com isso, os servi√ßos data_extractor e PostgreSQL estar√£o funcionando. √â poss√≠vel executar as pipelines manualmente com os seguintes comandos:

```
docker-compose run --rm app historic_extractor/run.py <C√ìDIGO DA EMPRESA>

# Exemplo: Coleta dados da VALE3.SA para a tabela tb_historic
docker-compose run --rm app historic_extractor/run.py VALE3.SA
```

```
docker-compose run --rm app real_time_extractor/run.py <C√ìDIGO DA EMPRESA>

# Exemplo: Coleta dados da VALE3.SA para a tabela tb_real_time
docker-compose run --rm app real_time_extractor/run.py VALE3.SA
```

```
docker-compose run --rm app general_information_extractor/run.py <LINK INFOMONEY DA EMPRESA> <C√ìDIGO DA EMPRESA>
# Exemplo: Coleta dados da VALE3.SA para a tabela tb_general_financials
docker-compose run --rm app general_information_extractor/run.py https://www.infomoney.com.br/cotacoes/b3/acao/vale-vale3/ VALE3.SA
```

### 2. Configura√ß√£o do Airflow

O Airflow √© configurado localmente com o airflowctl. Para instal√°-lo, para instalar, execute:

```
pip install airflowctl
```

```
airflowctl init airflow_project --build-start
```

Em seguida, mova o conte√∫do do diret√≥rio dags-temp da raiz para o diret√≥rio airflow_project/dags:

- general_etl.py
- historic_etl.py
- real_time_etl.py

Por fim, acesse localhost:8080, fa√ßa login com o usu√°rio e senha criados pelo Airflow (informados no terminal), e ative as DAGs para come√ßar a execu√ß√£o.

Caso as Dags n√£o apare√ßam, encerre o airflow para iniciar novamente:

```
airflowctl start airflow_project/
```
